{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR     # for dynamic learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from transformers import Wav2Vec2ForSequenceClassification, AdamW\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2ForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
      "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
      "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
      "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
      "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
      "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
      "\n",
      "              class  \n",
      "0          dog_bark  \n",
      "1  children_playing  \n",
      "2  children_playing  \n",
      "3  children_playing  \n",
      "4  children_playing  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8732 entries, 0 to 8731\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   slice_file_name  8732 non-null   object \n",
      " 1   fsID             8732 non-null   int64  \n",
      " 2   start            8732 non-null   float64\n",
      " 3   end              8732 non-null   float64\n",
      " 4   salience         8732 non-null   int64  \n",
      " 5   fold             8732 non-null   int64  \n",
      " 6   classID          8732 non-null   int64  \n",
      " 7   class            8732 non-null   object \n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 545.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('urbansound8k/metadata/UrbanSound8K.csv')\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "sns.countplot(x=df['classID'])\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = []\n",
    "labels = []\n",
    "\n",
    "label_dict = {}\n",
    "with open(\"urbansound8k/metadata/UrbanSound8K.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        label_dict[row[0]] = row[7]\n",
    "\n",
    "for fold in range(1, 11):  # UrbanSound8K contains data in 10 folds   relative path ---> UrbanSound8K/audio\n",
    "    fold_path = f\"UrbanSound8K/audio/fold{fold}/\"\n",
    "    for filename in os.listdir(fold_path):\n",
    "        if filename.endswith('.wav'):\n",
    "            file_path = os.path.join(fold_path, filename)\n",
    "            data, sr = librosa.load(file_path, sr=None)\n",
    "            audio_files.append((data, sr))\n",
    "            labels.append(label_dict[filename])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Classification using Wave2Vec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Pretrained Wave2Vec2 with classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guneeshvats/miniconda3/envs/adalat_v3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Wave2Vec2 model with a classification head\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=10)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Define target sampling rate\n",
    "TARGET_SR = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepating the dataset and predicting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)  # Encode string labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 6, 0, 0, 7, 7, 4, 7, 5, 4, 3, 7, 7, 8, 7, 5, 4, 8, 9, 4, 2, 7,\n",
       "       8, 7, 8, 8, 9, 2, 7, 3, 9, 6, 8, 9, 9, 5, 7, 7, 8, 9, 5, 4, 2, 7,\n",
       "       4, 0, 3, 7, 2, 0, 3, 7, 4, 2, 0, 3, 0, 7, 0, 0, 7, 4, 3, 4, 8, 7,\n",
       "       4, 5, 3, 2, 7, 7, 2, 7, 8, 6, 9, 7, 2, 7, 4, 8, 3, 7, 2, 4, 7, 8,\n",
       "       7, 5, 2, 9, 3, 4, 8, 7, 7, 9, 4, 2, 7, 7, 0, 0, 0, 3, 3, 0, 3, 0,\n",
       "       7, 0, 0, 3, 3, 7, 2, 7, 7, 9, 8, 3, 5, 5, 7, 4, 9, 9, 3, 7, 7, 3,\n",
       "       8, 9, 9, 8, 7, 4, 5, 7, 5, 7, 6, 7, 8, 7, 3, 6, 9, 0, 0, 7, 1, 3,\n",
       "       2, 6, 3, 0, 7, 3, 0, 6, 5, 1, 3, 3, 3, 2, 3, 3, 7, 4, 8, 8, 7, 9,\n",
       "       9, 3, 5, 7, 5, 1, 1, 7, 7, 8, 8, 3, 9, 7, 3, 7, 5, 7, 2, 5, 3, 8,\n",
       "       9, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess audio files and get zero-shot predictions\n",
    "zero_shot_preds = []\n",
    "true_labels = []\n",
    "\n",
    "for i, (data, sr) in enumerate(audio_files):\n",
    "    # Resample if necessary\n",
    "    if sr != TARGET_SR:\n",
    "        data = librosa.resample(data, orig_sr=sr, target_sr=TARGET_SR)\n",
    "    \n",
    "    # Preprocess with the processor\n",
    "    inputs = processor(data, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Perform inference without fine-tuning\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "        zero_shot_preds.append(predicted_label)\n",
    "    \n",
    "    # Store true label\n",
    "    true_labels.append(y[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Zero Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Classification:\n",
      "Accuracy: 0.14\n",
      "Precision: 0.07\n",
      "Recall: 0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guneeshvats/miniconda3/envs/adalat_v3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "zero_shot_accuracy = accuracy_score(true_labels, zero_shot_preds)\n",
    "zero_shot_precision = precision_score(true_labels, zero_shot_preds, average=\"macro\")\n",
    "zero_shot_recall = recall_score(true_labels, zero_shot_preds, average=\"macro\")\n",
    "\n",
    "# Display results\n",
    "print(\"Zero-Shot Classification:\")\n",
    "print(f\"Accuracy: {zero_shot_accuracy:.2f}\")\n",
    "print(f\"Precision: {zero_shot_precision:.2f}\")\n",
    "print(f\"Recall: {zero_shot_recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning with LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 6985\n",
      "Number of test samples: 1747\n"
     ]
    }
   ],
   "source": [
    "# Split audio_files and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(audio_files, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Number of training samples: {len(X_train)}\")\n",
    "print(f\"Number of test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a properly formatted dataset\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, audio_files, labels, processor, target_sr=16000, max_length=64000):\n",
    "        self.audio_files = audio_files\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.target_sr = target_sr\n",
    "        self.max_length = max_length  # Maximum length to pad/truncate audio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, sr = self.audio_files[idx]\n",
    "        \n",
    "        # Resample audio to target sample rate\n",
    "        if sr != self.target_sr:\n",
    "            data = librosa.resample(data, orig_sr=sr, target_sr=self.target_sr)\n",
    "        \n",
    "        # Pad or truncate audio to max_length\n",
    "        if len(data) > self.max_length:\n",
    "            data = data[:self.max_length]\n",
    "        else:\n",
    "            data = np.pad(data, (0, self.max_length - len(data)), mode='constant')\n",
    "        \n",
    "        # Preprocess audio and create input\n",
    "        inputs = self.processor(data, sampling_rate=self.target_sr, return_tensors=\"pt\", padding=True)\n",
    "        return {\n",
    "            \"input_values\": inputs[\"input_values\"].squeeze(0),  # Ensure correct format\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating DataLoader...\n",
      "Input Values Shape: torch.Size([8, 64000])\n",
      "Labels Shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(audio_files, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define UrbanSoundDataset and DataLoader\n",
    "train_dataset = UrbanSoundDataset(X_train, y_train, processor)\n",
    "test_dataset = UrbanSoundDataset(X_test, y_test, processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Validate the DataLoader\n",
    "print(\"Validating DataLoader...\")\n",
    "for batch in train_loader:\n",
    "    print(f\"Input Values Shape: {batch['input_values'].shape}\")  # Should be [batch_size, max_length]\n",
    "    print(f\"Labels Shape: {batch['labels'].shape}\")  # Should be [batch_size]\n",
    "    break  # Check only the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_values', 'labels'])\n",
      "torch.Size([8, 64000])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch.keys())  # Should include 'input_values' and 'labels'\n",
    "print(batch[\"input_values\"].shape)  # Expected: [batch_size, max_length]\n",
    "print(batch[\"labels\"].shape)  # Expected: [batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning using LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(2.2925, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0100, -0.0136,  0.0152,  0.0107,  0.0368, -0.0037, -0.0478, -0.0381,\n",
      "          0.0099,  0.0266],\n",
      "        [-0.0430, -0.0375, -0.0329, -0.0108,  0.0164, -0.0601, -0.0472, -0.0272,\n",
      "         -0.0067, -0.0025],\n",
      "        [-0.0783, -0.0334, -0.0290, -0.0038,  0.0368, -0.0750, -0.0618, -0.0062,\n",
      "         -0.0010, -0.0207],\n",
      "        [-0.0185, -0.0049, -0.0316, -0.0127,  0.0087, -0.0181, -0.0125, -0.0306,\n",
      "          0.0088,  0.0042],\n",
      "        [-0.0420, -0.0136, -0.0389, -0.0099,  0.0130, -0.0452, -0.0278, -0.0258,\n",
      "          0.0175,  0.0212],\n",
      "        [-0.0191, -0.0148, -0.0074,  0.0081,  0.0171, -0.0138, -0.0220, -0.0370,\n",
      "          0.0217,  0.0228],\n",
      "        [-0.0536, -0.0182, -0.0079,  0.0038,  0.0241, -0.0523, -0.0286, -0.0231,\n",
      "          0.0051, -0.0055],\n",
      "        [-0.0533, -0.0440, -0.0340, -0.0151,  0.0185, -0.0712, -0.0576, -0.0244,\n",
      "         -0.0185,  0.0031]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Testing the base model :\n",
    "# Load the base model without LoRA\n",
    "base_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=10)\n",
    "base_model.to(device)\n",
    "\n",
    "# Test a forward pass with the base model\n",
    "outputs = base_model(input_values=batch[\"input_values\"].to(device), labels=batch[\"labels\"].to(device))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wav2vec2\n",
      "wav2vec2.feature_extractor\n",
      "wav2vec2.feature_extractor.conv_layers\n",
      "wav2vec2.feature_extractor.conv_layers.0\n",
      "wav2vec2.feature_extractor.conv_layers.0.conv\n",
      "wav2vec2.feature_extractor.conv_layers.0.activation\n",
      "wav2vec2.feature_extractor.conv_layers.0.layer_norm\n",
      "wav2vec2.feature_extractor.conv_layers.1\n",
      "wav2vec2.feature_extractor.conv_layers.1.conv\n",
      "wav2vec2.feature_extractor.conv_layers.1.activation\n",
      "wav2vec2.feature_extractor.conv_layers.2\n",
      "wav2vec2.feature_extractor.conv_layers.2.conv\n",
      "wav2vec2.feature_extractor.conv_layers.2.activation\n",
      "wav2vec2.feature_extractor.conv_layers.3\n",
      "wav2vec2.feature_extractor.conv_layers.3.conv\n",
      "wav2vec2.feature_extractor.conv_layers.3.activation\n",
      "wav2vec2.feature_extractor.conv_layers.4\n",
      "wav2vec2.feature_extractor.conv_layers.4.conv\n",
      "wav2vec2.feature_extractor.conv_layers.4.activation\n",
      "wav2vec2.feature_extractor.conv_layers.5\n",
      "wav2vec2.feature_extractor.conv_layers.5.conv\n",
      "wav2vec2.feature_extractor.conv_layers.5.activation\n",
      "wav2vec2.feature_extractor.conv_layers.6\n",
      "wav2vec2.feature_extractor.conv_layers.6.conv\n",
      "wav2vec2.feature_extractor.conv_layers.6.activation\n",
      "wav2vec2.feature_projection\n",
      "wav2vec2.feature_projection.layer_norm\n",
      "wav2vec2.feature_projection.projection\n",
      "wav2vec2.feature_projection.dropout\n",
      "wav2vec2.encoder\n",
      "wav2vec2.encoder.pos_conv_embed\n",
      "wav2vec2.encoder.pos_conv_embed.conv\n",
      "wav2vec2.encoder.pos_conv_embed.conv.parametrizations\n",
      "wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight\n",
      "wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.0\n",
      "wav2vec2.encoder.pos_conv_embed.padding\n",
      "wav2vec2.encoder.pos_conv_embed.activation\n",
      "wav2vec2.encoder.layer_norm\n",
      "wav2vec2.encoder.dropout\n",
      "wav2vec2.encoder.layers\n",
      "wav2vec2.encoder.layers.0\n",
      "wav2vec2.encoder.layers.0.attention\n",
      "wav2vec2.encoder.layers.0.attention.k_proj\n",
      "wav2vec2.encoder.layers.0.attention.v_proj\n",
      "wav2vec2.encoder.layers.0.attention.q_proj\n",
      "wav2vec2.encoder.layers.0.attention.out_proj\n",
      "wav2vec2.encoder.layers.0.dropout\n",
      "wav2vec2.encoder.layers.0.layer_norm\n",
      "wav2vec2.encoder.layers.0.feed_forward\n",
      "wav2vec2.encoder.layers.0.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.0.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.0.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.0.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.0.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.0.final_layer_norm\n",
      "wav2vec2.encoder.layers.1\n",
      "wav2vec2.encoder.layers.1.attention\n",
      "wav2vec2.encoder.layers.1.attention.k_proj\n",
      "wav2vec2.encoder.layers.1.attention.v_proj\n",
      "wav2vec2.encoder.layers.1.attention.q_proj\n",
      "wav2vec2.encoder.layers.1.attention.out_proj\n",
      "wav2vec2.encoder.layers.1.dropout\n",
      "wav2vec2.encoder.layers.1.layer_norm\n",
      "wav2vec2.encoder.layers.1.feed_forward\n",
      "wav2vec2.encoder.layers.1.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.1.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.1.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.1.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.1.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.1.final_layer_norm\n",
      "wav2vec2.encoder.layers.2\n",
      "wav2vec2.encoder.layers.2.attention\n",
      "wav2vec2.encoder.layers.2.attention.k_proj\n",
      "wav2vec2.encoder.layers.2.attention.v_proj\n",
      "wav2vec2.encoder.layers.2.attention.q_proj\n",
      "wav2vec2.encoder.layers.2.attention.out_proj\n",
      "wav2vec2.encoder.layers.2.dropout\n",
      "wav2vec2.encoder.layers.2.layer_norm\n",
      "wav2vec2.encoder.layers.2.feed_forward\n",
      "wav2vec2.encoder.layers.2.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.2.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.2.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.2.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.2.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.2.final_layer_norm\n",
      "wav2vec2.encoder.layers.3\n",
      "wav2vec2.encoder.layers.3.attention\n",
      "wav2vec2.encoder.layers.3.attention.k_proj\n",
      "wav2vec2.encoder.layers.3.attention.v_proj\n",
      "wav2vec2.encoder.layers.3.attention.q_proj\n",
      "wav2vec2.encoder.layers.3.attention.out_proj\n",
      "wav2vec2.encoder.layers.3.dropout\n",
      "wav2vec2.encoder.layers.3.layer_norm\n",
      "wav2vec2.encoder.layers.3.feed_forward\n",
      "wav2vec2.encoder.layers.3.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.3.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.3.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.3.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.3.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.3.final_layer_norm\n",
      "wav2vec2.encoder.layers.4\n",
      "wav2vec2.encoder.layers.4.attention\n",
      "wav2vec2.encoder.layers.4.attention.k_proj\n",
      "wav2vec2.encoder.layers.4.attention.v_proj\n",
      "wav2vec2.encoder.layers.4.attention.q_proj\n",
      "wav2vec2.encoder.layers.4.attention.out_proj\n",
      "wav2vec2.encoder.layers.4.dropout\n",
      "wav2vec2.encoder.layers.4.layer_norm\n",
      "wav2vec2.encoder.layers.4.feed_forward\n",
      "wav2vec2.encoder.layers.4.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.4.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.4.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.4.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.4.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.4.final_layer_norm\n",
      "wav2vec2.encoder.layers.5\n",
      "wav2vec2.encoder.layers.5.attention\n",
      "wav2vec2.encoder.layers.5.attention.k_proj\n",
      "wav2vec2.encoder.layers.5.attention.v_proj\n",
      "wav2vec2.encoder.layers.5.attention.q_proj\n",
      "wav2vec2.encoder.layers.5.attention.out_proj\n",
      "wav2vec2.encoder.layers.5.dropout\n",
      "wav2vec2.encoder.layers.5.layer_norm\n",
      "wav2vec2.encoder.layers.5.feed_forward\n",
      "wav2vec2.encoder.layers.5.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.5.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.5.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.5.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.5.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.5.final_layer_norm\n",
      "wav2vec2.encoder.layers.6\n",
      "wav2vec2.encoder.layers.6.attention\n",
      "wav2vec2.encoder.layers.6.attention.k_proj\n",
      "wav2vec2.encoder.layers.6.attention.v_proj\n",
      "wav2vec2.encoder.layers.6.attention.q_proj\n",
      "wav2vec2.encoder.layers.6.attention.out_proj\n",
      "wav2vec2.encoder.layers.6.dropout\n",
      "wav2vec2.encoder.layers.6.layer_norm\n",
      "wav2vec2.encoder.layers.6.feed_forward\n",
      "wav2vec2.encoder.layers.6.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.6.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.6.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.6.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.6.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.6.final_layer_norm\n",
      "wav2vec2.encoder.layers.7\n",
      "wav2vec2.encoder.layers.7.attention\n",
      "wav2vec2.encoder.layers.7.attention.k_proj\n",
      "wav2vec2.encoder.layers.7.attention.v_proj\n",
      "wav2vec2.encoder.layers.7.attention.q_proj\n",
      "wav2vec2.encoder.layers.7.attention.out_proj\n",
      "wav2vec2.encoder.layers.7.dropout\n",
      "wav2vec2.encoder.layers.7.layer_norm\n",
      "wav2vec2.encoder.layers.7.feed_forward\n",
      "wav2vec2.encoder.layers.7.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.7.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.7.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.7.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.7.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.7.final_layer_norm\n",
      "wav2vec2.encoder.layers.8\n",
      "wav2vec2.encoder.layers.8.attention\n",
      "wav2vec2.encoder.layers.8.attention.k_proj\n",
      "wav2vec2.encoder.layers.8.attention.v_proj\n",
      "wav2vec2.encoder.layers.8.attention.q_proj\n",
      "wav2vec2.encoder.layers.8.attention.out_proj\n",
      "wav2vec2.encoder.layers.8.dropout\n",
      "wav2vec2.encoder.layers.8.layer_norm\n",
      "wav2vec2.encoder.layers.8.feed_forward\n",
      "wav2vec2.encoder.layers.8.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.8.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.8.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.8.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.8.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.8.final_layer_norm\n",
      "wav2vec2.encoder.layers.9\n",
      "wav2vec2.encoder.layers.9.attention\n",
      "wav2vec2.encoder.layers.9.attention.k_proj\n",
      "wav2vec2.encoder.layers.9.attention.v_proj\n",
      "wav2vec2.encoder.layers.9.attention.q_proj\n",
      "wav2vec2.encoder.layers.9.attention.out_proj\n",
      "wav2vec2.encoder.layers.9.dropout\n",
      "wav2vec2.encoder.layers.9.layer_norm\n",
      "wav2vec2.encoder.layers.9.feed_forward\n",
      "wav2vec2.encoder.layers.9.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.9.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.9.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.9.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.9.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.9.final_layer_norm\n",
      "wav2vec2.encoder.layers.10\n",
      "wav2vec2.encoder.layers.10.attention\n",
      "wav2vec2.encoder.layers.10.attention.k_proj\n",
      "wav2vec2.encoder.layers.10.attention.v_proj\n",
      "wav2vec2.encoder.layers.10.attention.q_proj\n",
      "wav2vec2.encoder.layers.10.attention.out_proj\n",
      "wav2vec2.encoder.layers.10.dropout\n",
      "wav2vec2.encoder.layers.10.layer_norm\n",
      "wav2vec2.encoder.layers.10.feed_forward\n",
      "wav2vec2.encoder.layers.10.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.10.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.10.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.10.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.10.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.10.final_layer_norm\n",
      "wav2vec2.encoder.layers.11\n",
      "wav2vec2.encoder.layers.11.attention\n",
      "wav2vec2.encoder.layers.11.attention.k_proj\n",
      "wav2vec2.encoder.layers.11.attention.v_proj\n",
      "wav2vec2.encoder.layers.11.attention.q_proj\n",
      "wav2vec2.encoder.layers.11.attention.out_proj\n",
      "wav2vec2.encoder.layers.11.dropout\n",
      "wav2vec2.encoder.layers.11.layer_norm\n",
      "wav2vec2.encoder.layers.11.feed_forward\n",
      "wav2vec2.encoder.layers.11.feed_forward.intermediate_dropout\n",
      "wav2vec2.encoder.layers.11.feed_forward.intermediate_dense\n",
      "wav2vec2.encoder.layers.11.feed_forward.intermediate_act_fn\n",
      "wav2vec2.encoder.layers.11.feed_forward.output_dense\n",
      "wav2vec2.encoder.layers.11.feed_forward.output_dropout\n",
      "wav2vec2.encoder.layers.11.final_layer_norm\n",
      "projector\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, module in base_model.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = [\n",
    "    \"wav2vec2.encoder.layers.0.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.0.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.0.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.0.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.0.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.0.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.1.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.1.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.1.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.1.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.1.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.1.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.2.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.2.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.2.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.2.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.2.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.2.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.3.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.3.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.3.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.3.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.3.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.3.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.4.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.4.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.4.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.4.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.4.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.4.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.5.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.5.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.5.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.5.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.5.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.5.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.6.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.6.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.6.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.6.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.6.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.6.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.7.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.7.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.7.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.7.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.7.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.7.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.8.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.8.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.8.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.8.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.8.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.8.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.9.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.9.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.9.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.9.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.9.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.9.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.10.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.10.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.10.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.10.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.10.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.10.feed_forward.output_dense\",\n",
    "    \"wav2vec2.encoder.layers.11.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.11.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.11.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.11.attention.out_proj\",\n",
    "    \"wav2vec2.encoder.layers.11.feed_forward.intermediate_dense\",\n",
    "    \"wav2vec2.encoder.layers.11.feed_forward.output_dense\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guneeshvats/miniconda3/envs/adalat_v3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 51,722 || all params: 94,622,868 || trainable%: 0.0547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/guneeshvats/miniconda3/envs/adalat_v3/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 874/874 [1:13:58<00:00,  5.08s/batch, Batch Loss=2.09]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.1501\n",
      "Validation Loss: 2.0672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vx/9cf9tngn6qs563bg7n_qlt5m0000gn/T/ipykernel_12195/571521404.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=10)\n",
    "model.to(device)\n",
    "\n",
    "# LoRA Configuration\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "target_modules = [\n",
    "    \"wav2vec2.encoder.layers.0.attention.k_proj\",\n",
    "    \"wav2vec2.encoder.layers.0.attention.q_proj\",\n",
    "    \"wav2vec2.encoder.layers.0.attention.v_proj\",\n",
    "    \"wav2vec2.encoder.layers.0.attention.out_proj\",\n",
    "]\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, config)\n",
    "model.to(device)\n",
    "# Verifying trainable parameters\n",
    "model.print_trainable_parameters() \n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=10)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping_patience = 3\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Lists to store training and validation loss\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Halving Learning rate after every epoch   (can change the step size)\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 1  # Can Increase epochs for meaningful results\n",
    "val_loader = test_loader  # Rename for clarity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", unit=\"batch\") as pbar:\n",
    "        for batch in pbar:\n",
    "            inputs = {\n",
    "                \"input_values\": batch[\"input_values\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            }\n",
    "            outputs = model(\n",
    "                input_values=inputs[\"input_values\"],\n",
    "                labels=inputs[\"labels\"],\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    print(f\"Training Loss: {train_losses[-1]:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {\n",
    "                \"input_values\": batch[\"input_values\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            }\n",
    "            outputs = model(\n",
    "                input_values=inputs[\"input_values\"],\n",
    "                labels=inputs[\"labels\"],\n",
    "            )\n",
    "            val_loss += outputs.loss.item()\n",
    "            val_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            val_labels.extend(inputs[\"labels\"].cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Values Shape: torch.Size([8, 64000])\n",
      "Labels Shape: torch.Size([8])\n",
      "None\n",
      "trainable params: 1,329,674 || all params: 95,900,820 || trainable%: 1.3865\n",
      "odict_keys(['base_model.model.wav2vec2.masked_spec_embed', 'base_model.model.wav2vec2.feature_extractor.conv_layers.0.conv.weight', 'base_model.model.wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight', 'base_model.model.wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias', 'base_model.model.wav2vec2.feature_extractor.conv_layers.1.conv.weight', 'base_model.model.wav2vec2.feature_extractor.conv_layers.2.conv.weight', 'base_model.model.wav2vec2.feature_extractor.conv_layers.3.conv.weight', 'base_model.model.wav2vec2.feature_extractor.conv_layers.4.conv.weight', 'base_model.model.wav2vec2.feature_extractor.conv_layers.5.conv.weight', 'base_model.model.wav2vec2.feature_extractor.conv_layers.6.conv.weight', 'base_model.model.wav2vec2.feature_projection.layer_norm.weight', 'base_model.model.wav2vec2.feature_projection.layer_norm.bias', 'base_model.model.wav2vec2.feature_projection.projection.weight', 'base_model.model.wav2vec2.feature_projection.projection.bias', 'base_model.model.wav2vec2.encoder.pos_conv_embed.conv.bias', 'base_model.model.wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'base_model.model.wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'base_model.model.wav2vec2.encoder.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.0.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.0.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.0.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.0.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.0.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.0.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.0.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.0.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.0.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.1.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.1.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.1.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.1.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.1.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.1.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.1.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.1.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.1.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.2.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.2.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.2.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.2.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.2.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.2.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.2.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.2.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.2.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.3.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.3.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.3.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.3.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.3.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.3.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.3.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.3.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.3.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.4.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.4.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.4.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.4.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.4.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.4.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.4.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.4.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.4.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.5.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.5.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.5.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.5.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.5.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.5.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.5.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.5.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.5.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.6.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.6.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.6.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.6.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.6.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.6.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.6.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.6.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.6.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.7.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.7.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.7.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.7.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.7.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.7.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.7.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.7.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.7.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.7.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.8.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.8.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.8.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.8.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.8.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.8.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.8.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.8.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.8.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.8.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.9.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.9.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.9.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.9.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.9.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.9.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.9.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.9.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.9.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.9.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.10.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.10.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.10.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.10.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.10.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.10.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.10.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.10.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.10.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.10.final_layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.11.attention.k_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.k_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.11.attention.k_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.k_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.v_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.v_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.11.attention.v_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.v_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.q_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.q_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.11.attention.q_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.q_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.out_proj.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.out_proj.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.11.attention.out_proj.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.attention.out_proj.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.11.layer_norm.bias', 'base_model.model.wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.feed_forward.output_dense.base_layer.weight', 'base_model.model.wav2vec2.encoder.layers.11.feed_forward.output_dense.base_layer.bias', 'base_model.model.wav2vec2.encoder.layers.11.feed_forward.output_dense.lora_A.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.feed_forward.output_dense.lora_B.default.weight', 'base_model.model.wav2vec2.encoder.layers.11.final_layer_norm.weight', 'base_model.model.wav2vec2.encoder.layers.11.final_layer_norm.bias', 'base_model.model.projector.weight', 'base_model.model.projector.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.classifier.modules_to_save.default.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Values Shape: {batch['input_values'].shape}\")\n",
    "print(f\"Labels Shape: {batch['labels'].shape}\")\n",
    "\n",
    "print(model.forward.__doc__)  # Check the forward method signature\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(model.state_dict().keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Classifier  Accuracy  Precision    Recall\n",
      "0  Wav2Vec2 + LoRA  0.195192   0.069279  0.161142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guneeshvats/miniconda3/envs/adalat_v3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "test_preds, test_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {\n",
    "            \"input_values\": batch[\"input_values\"].to(device),\n",
    "            \"labels\": batch[\"labels\"].to(device),\n",
    "        }\n",
    "        outputs = model(\n",
    "            input_values=inputs[\"input_values\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        )\n",
    "        test_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        test_labels.extend(inputs[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision = precision_score(test_labels, test_preds, average=\"macro\")\n",
    "recall = recall_score(test_labels, test_preds, average=\"macro\")\n",
    "\n",
    "# Create a table\n",
    "results = pd.DataFrame({\n",
    "    \"Classifier\": [\"Wav2Vec2 + LoRA\"],\n",
    "    \"Accuracy\": [accuracy],\n",
    "    \"Precision\": [precision],\n",
    "    \"Recall\": [recall]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame({\n",
    "#     \"Method\": [\"Layer-Freezing Fine-Tuning\", \"LoRA Fine-Tuning\"],\n",
    "#     \"Accuracy\": [layer_freezing_accuracy * 100, lora_accuracy * 100],\n",
    "#     \"Precision\": [layer_freezing_precision * 100, lora_precision * 100],\n",
    "#     \"Recall\": [layer_freezing_recall * 100, lora_recall * 100]\n",
    "# })\n",
    "\n",
    "# # Display results\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coparison between ZSC Model and Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guneeshvats/miniconda3/envs/adalat_v3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Zero-Shot Model  Fine-Tuned Model\n",
      "0   Accuracy         0.195192          0.195192\n",
      "1  Precision         0.069279          0.069279\n",
      "2     Recall         0.161142          0.161142\n",
      "3   F1-Score         0.075213          0.075213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guneeshvats/miniconda3/envs/adalat_v3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluate Zero-Shot Model\n",
    "zero_shot_preds, zero_shot_labels = [], []\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {\n",
    "            \"input_values\": batch[\"input_values\"].to(device),\n",
    "        }\n",
    "        outputs = model(\n",
    "            input_values=inputs[\"input_values\"]\n",
    "        )\n",
    "        zero_shot_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        zero_shot_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Zero-Shot Metrics\n",
    "zs_accuracy = accuracy_score(zero_shot_labels, zero_shot_preds)\n",
    "zs_precision = precision_score(zero_shot_labels, zero_shot_preds, average=\"macro\")\n",
    "zs_recall = recall_score(zero_shot_labels, zero_shot_preds, average=\"macro\")\n",
    "zs_f1 = f1_score(zero_shot_labels, zero_shot_preds, average=\"macro\")\n",
    "\n",
    "# Evaluate Fine-Tuned Model\n",
    "fine_tuned_preds, fine_tuned_labels = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {\n",
    "            \"input_values\": batch[\"input_values\"].to(device),\n",
    "            \"labels\": batch[\"labels\"].to(device),\n",
    "        }\n",
    "        outputs = model(\n",
    "            input_values=inputs[\"input_values\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        )\n",
    "        fine_tuned_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        fine_tuned_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Fine-Tuned Metrics\n",
    "ft_accuracy = accuracy_score(fine_tuned_labels, fine_tuned_preds)\n",
    "ft_precision = precision_score(fine_tuned_labels, fine_tuned_preds, average=\"macro\")\n",
    "ft_recall = recall_score(fine_tuned_labels, fine_tuned_preds, average=\"macro\")\n",
    "ft_f1 = f1_score(fine_tuned_labels, fine_tuned_preds, average=\"macro\")\n",
    "\n",
    "# Combine Results into a Table\n",
    "comparison_table = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n",
    "    \"Zero-Shot Model\": [zs_accuracy, zs_precision, zs_recall, zs_f1],\n",
    "    \"Fine-Tuned Model\": [ft_accuracy, ft_precision, ft_recall, ft_f1],\n",
    "})\n",
    "\n",
    "# Display the Table\n",
    "print(comparison_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and processor\n",
    "model.save_pretrained(\"./fine_tuned_wav2vec2\")\n",
    "processor.save_pretrained(\"./fine_tuned_wav2vec2\")\n",
    "\n",
    "print(\"Fine-tuned model and processor saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load the model you can use this \n",
    "\n",
    "`from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor`\n",
    "\n",
    "`model = Wav2Vec2ForSequenceClassification.from_pretrained(\"./fine_tuned_wav2vec2\")`\n",
    "`processor = Wav2Vec2Processor.from_pretrained(\"./fine_tuned_wav2vec2\")`\n",
    "\n",
    "`print(\"Fine-tuned model and processor loaded successfully!\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF ASSIGNMENT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adalat_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
